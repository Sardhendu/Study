{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: (200000, 28, 28) (200000,)\n",
      "Cross Validation: (9810, 28, 28) (9810,)\n",
      "Testing: (7709, 28, 28) (7709,)\n"
     ]
    }
   ],
   "source": [
    "# Now as always we get the data we stored in the disk.\n",
    "cleaned_dataset_path = '/Users/sam/All-Program/App-DataSet/Deep-Neural-Nets/Udacity/DataPreparation/dataset_cleaned.p'\n",
    "\n",
    "with open(cleaned_dataset_path, 'rb') as f:\n",
    "    fnl_dataset = pickle.load(f)\n",
    "    training_dataset = (fnl_dataset['training_dataset'])\n",
    "    training_labels = (fnl_dataset['training_labels'])\n",
    "    test_dataset = (fnl_dataset['test_dataset'])\n",
    "    test_labels = (fnl_dataset['test_labels'])\n",
    "    crossvalid_dataset = (fnl_dataset['crossvalid_dataset'])\n",
    "    crossvalid_labels = (fnl_dataset['crossvalid_labels'])\n",
    "    \n",
    "print('Training:', training_dataset.shape, training_labels.shape)\n",
    "print('Cross Validation:', crossvalid_dataset.shape, crossvalid_labels.shape)\n",
    "print('Testing:', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of label 0 is : 20000\n",
      "Total number of label 1 is : 20000\n",
      "Total number of label 2 is : 20000\n",
      "Total number of label 3 is : 20000\n",
      "Total number of label 4 is : 20000\n",
      "Total number of label 5 is : 20000\n",
      "Total number of label 6 is : 20000\n",
      "Total number of label 7 is : 20000\n",
      "Total number of label 8 is : 20000\n",
      "Total number of label 9 is : 20000\n",
      "the batch size is: 1000\n"
     ]
    }
   ],
   "source": [
    "# Mini-Batch Creation\n",
    "no_of_batches = 200\n",
    "batch_indices = []\n",
    "def batch_indice_creator(training_labels, no_of_batches):\n",
    "    \"\"\"Create batch with random samples and return appropriate format\"\"\"\n",
    "    label_rand_arr = []\n",
    "    no_of_labels = len(np.unique(training_labels))\n",
    "    \n",
    "    if len(training_labels)/no_of_batches < no_of_labels:\n",
    "        raise Exception (\"Every batch should contain atleast 1 example from all the differnt class labels\")\n",
    "    else:\n",
    "        for unq_labels in np.unique(training_labels):\n",
    "            arr = np.array(np.where(training_labels==unq_labels)[0])\n",
    "            print ('Total number of label %d is :'%unq_labels, len(arr))\n",
    "            np.random.shuffle(arr)\n",
    "    #         print ('the random array is: ', np.reshape(arr, (len(arr),1)))\n",
    "            if np.any(label_rand_arr):\n",
    "                label_rand_arr = np.hstack((label_rand_arr, np.reshape(arr, (len(arr),1))))\n",
    "            else:\n",
    "                label_rand_arr = np.reshape(arr, (len(arr),1))\n",
    "        \n",
    "        no_of_batches_new = no_of_batches\n",
    "        if label_rand_arr.shape[0]%no_of_batches!=0:\n",
    "            no_of_batches_new = no_of_batches + label_rand_arr.shape[0]%no_of_batches\n",
    "            print ('Provided no_of_batches doesnt distribute the labels equally, ne no_of_batches is: ', no_of_batches_new) \n",
    "        batch_indices = np.array_split(label_rand_arr.flatten(), no_of_batches_new)\n",
    "        batch_size = len(batch_indices[0])\n",
    "    return batch_indices, batch_size\n",
    "\n",
    "\n",
    "training_batch_dir = '/Users/sam/All-Program/App-DataSet/Deep-Neural-Nets/Tensor-Flow-Learn/DataPrepared/'        \n",
    "batch_training_dict = {}\n",
    "def batch_store(training_dataset, training_labels, batch_indices):\n",
    "    for no, indices in enumerate(batch_indices):\n",
    "        #print (training_labels[indices])\n",
    "        try:\n",
    "            f = open(training_batch_dir+'batch'+str(no)+'.pickle', 'wb')\n",
    "            batch = {\n",
    "                'batch_train_dataset': training_dataset[indices],\n",
    "                'batch_train_labels': training_labels[indices],\n",
    "            }\n",
    "            pickle.dump(batch, f, pickle.HIGHEST_PROTOCOL)\n",
    "            f.close()\n",
    "        except Exception as e:\n",
    "            print('Unable to save data to', complete_dataset_path, ':', e)\n",
    "            raise\n",
    "            \n",
    "batch_indices, batch_size = batch_indice_creator(training_labels, no_of_batches)  #print ([arr.shape for arr in batch_indices])\n",
    "#batch_store(training_dataset, training_labels, batch_indices)\n",
    "\n",
    "print ('the batch size is:',batch_size )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation set (9810, 784) (9810, 10)\n",
      "Test set (7709, 784) (7709, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "no_of_labels = 10\n",
    "no_inp_unit = image_size * image_size\n",
    "\n",
    "# tensor flow takes the labels input as binary code, where Alphabet A whose binary value is 0 will turn to a array\n",
    "# with elements [1.0, 0, 0, 0, 0, 0, 0, 0, 0, 0] and B becomes [0, 1.0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "def reshape_data(dataset, labels, sample_size=None):\n",
    "    if sample_size:\n",
    "        dataset = dataset[:sample_size].reshape(sample_size, no_inp_unit) # To reshape the  \n",
    "        # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "        labels = (np.arange(no_of_labels) == labels[:,None]).astype(np.float32)\n",
    "    else:\n",
    "        dataset = dataset.reshape(len(dataset), no_inp_unit) # To reshape the  \n",
    "        # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "        labels = (np.arange(no_of_labels) == labels[:,None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "\n",
    "# We just reshape the image so that 1 image defines a row\n",
    "\n",
    "crossvalid_dataset_, crossvalid_labels_ = reshape_data(crossvalid_dataset, crossvalid_labels)\n",
    "test_dataset_, test_labels_ = reshape_data(test_dataset, test_labels)\n",
    "print('Cross Validation set', crossvalid_dataset_.shape, crossvalid_labels_.shape)\n",
    "print('Test set', test_dataset_.shape, test_labels_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Building the Tensor Flow Graph\n",
    "learning_rate = 0.5\n",
    "momentum = 0.9\n",
    "no_hid_unit = 200\n",
    "no_output_unit = no_of_labels\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    seed = 128\n",
    "    rng = np.random.RandomState(seed)\n",
    "    tf_training_dataset = tf.placeholder(tf.float32, shape=(batch_size, no_inp_unit))\n",
    "    tf_training_labels = tf.placeholder(tf.float32, shape=(batch_size, no_of_labels))\n",
    "    tf_crossvalid_dataset = tf.constant(crossvalid_dataset_)\n",
    "    tf_crossvalid_labels = tf.constant(crossvalid_labels_)\n",
    "    tf_test_dataset = tf.constant(test_dataset_)\n",
    "    tf_test_labels = tf.constant(test_labels_)\n",
    "\n",
    "\n",
    "    weights = {\n",
    "        'input_to_hid_wghts': tf.Variable(tf.random_normal([no_inp_unit, no_hid_unit], seed=seed)),\n",
    "        'hid_to_output_wghts': tf.Variable(tf.random_normal([no_hid_unit, no_output_unit], seed=seed))\n",
    "    }\n",
    "\n",
    "    biases = {\n",
    "        'hid_bias' : tf.Variable(tf.zeros([no_hid_unit])),\n",
    "        'output_bias' : tf.Variable(tf.zeros([no_output_unit]))\n",
    "    }\n",
    "\n",
    "    ###### Forward Propagate ######\n",
    "    # Hidden Layer\n",
    "    input_to_hid_layer = tf.matmul(tf_training_dataset, weights['input_to_hid_wghts']) + biases['hid_bias']\n",
    "    hid_layer_state = tf.sigmoid(input_to_hid_layer, name=None)\n",
    "\n",
    "    # Output Layer\n",
    "    hid_to_output_layer = tf.matmul(hid_layer_state, weights['hid_to_output_wghts']) + biases['output_bias']\n",
    "    output_layer_state = tf.nn.softmax(hid_to_output_layer, name=None)\n",
    "    error_derivative = tf.sub(output_layer_state, tf_training_labels, name=None)  # -(y - y_hat) = (y_hat - y)\n",
    "    # The above three lines of code can also be combined into one line as below.\n",
    "    loss_CE = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(hid_to_output_layer, tf_training_labels))\n",
    "\n",
    "    #optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss_CE)\n",
    "    \n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, \n",
    "                                            momentum, \n",
    "                                            use_locking=False, \n",
    "                                            name='Momentum', \n",
    "                                            use_nesterov=True).minimize(loss_CE)\n",
    "    # Training Prediction\n",
    "    training_prediction = output_layer_state\n",
    "    # Cross-validation Prediction\n",
    "    cvd_hid_layer_state = tf.sigmoid(tf.matmul(tf_crossvalid_dataset, weights['input_to_hid_wghts']) + biases['hid_bias'], name=None)\n",
    "    crossvalid_prediction = tf.nn.softmax(tf.matmul(cvd_hid_layer_state, weights['hid_to_output_wghts']) + biases['output_bias'])\n",
    "    # Test Data Prediction\n",
    "    tst_hid_layer_state = tf.sigmoid(tf.matmul(tf_test_dataset, weights['input_to_hid_wghts']) + biases['hid_bias'], name=None)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tst_hid_layer_state, weights['hid_to_output_wghts']) + biases['output_bias'])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All variable Initialized successfully\n",
      "Minibatch loss at epoch 0: 0.771911\n",
      "Minibatch accuracy: 79.5%\n",
      "Validation accuracy: 59.2%\n",
      "Minibatch loss at epoch 2: 0.602523\n",
      "Minibatch accuracy: 82.7%\n",
      "Validation accuracy: 64.0%\n",
      "Minibatch loss at epoch 4: 0.565977\n",
      "Minibatch accuracy: 84.3%\n",
      "Validation accuracy: 65.2%\n",
      "Minibatch loss at epoch 6: 0.539594\n",
      "Minibatch accuracy: 84.6%\n",
      "Validation accuracy: 65.7%\n",
      "Minibatch loss at epoch 8: 0.517757\n",
      "Minibatch accuracy: 84.9%\n",
      "Validation accuracy: 66.0%\n",
      "Minibatch loss at epoch 9: 0.508369\n",
      "Minibatch accuracy: 85.3%\n",
      "Validation accuracy: 66.1%\n",
      "Test accuracy: 88.7%\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"All variable Initialized successfully\")\n",
    "    for epoch in range(epochs):\n",
    "        # Retreive mini-batches\n",
    "        for no in range(no_of_batches):\n",
    "            #print (no)\n",
    "            with open(training_batch_dir+'batch'+str(no)+'.pickle', 'rb') as f:\n",
    "                batch_dataset = pickle.load(f)\n",
    "                batch_data = (batch_dataset['batch_train_dataset'])\n",
    "                batch_labels = (batch_dataset['batch_train_labels'])\n",
    "                batch_data, batch_labels = reshape_data(batch_data, batch_labels)\n",
    "                \n",
    "                feed_dict = {tf_training_dataset : batch_data, tf_training_labels : batch_labels}\n",
    "                _, l, predictions = session.run([optimizer, loss_CE, training_prediction], feed_dict=feed_dict)\n",
    "        \n",
    "        if (epoch % 2 == 0) or epoch == epochs-1:\n",
    "            print(\"Minibatch loss at epoch %d: %f\" % (epoch, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(crossvalid_prediction.eval(), crossvalid_labels_))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Note :\n",
    "# For the momentum optimizer using Nesterov's method we get a very good accuracy rate\n",
    "# Minibatch accuracy: 85.3%  --> training accuracy\n",
    "# Validation accuracy: 66.1%\n",
    "# Test accuracy: 88.7%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normally Tensor flow does the Back-propogation automatically after you build the graph and run the \n",
    "# session. But by any chance if you want to implement your own backpropogation. then do the below.\n",
    "\n",
    "\n",
    "###########  Practise  #############\n",
    "\n",
    "# graph = tf.Graph()\n",
    "# with graph.as_default():\n",
    "\n",
    "#     seed = 128\n",
    "#     rng = np.random.RandomState(seed)\n",
    "#     tf_training_dataset = tf.placeholder(tf.float32, shape=(batch_size, no_inp_unit))\n",
    "#     tf_training_labels = tf.placeholder(tf.float32, shape=(batch_size, no_of_labels))\n",
    "# #     tf_training_dataset = tf.constant(training_dataset_)\n",
    "# #     tf_training_labels = tf.constant(training_labels_)\n",
    "\n",
    "#     weights = {\n",
    "#         'input_to_hid_wghts': tf.Variable(tf.random_normal([no_inp_unit, no_hid_unit], seed=seed)),\n",
    "#         'hid_to_output_wghts': tf.Variable(tf.random_normal([no_hid_unit, no_output_unit], seed=seed)),\n",
    "#         'input_to_hid_wghts_delta': tf.Variable(tf.zeros([no_inp_unit, no_hid_unit])),\n",
    "#         'hid_to_output_wghts_delta': tf.Variable(tf.zeros([no_hid_unit, no_output_unit])),\n",
    "#     }\n",
    "\n",
    "#     biases = {\n",
    "#         'hid_bias' : tf.Variable(tf.zeros([no_hid_unit])),\n",
    "#         'output_bias' : tf.Variable(tf.zeros([no_output_unit])),\n",
    "#         'hid_bias_delta' : tf.Variable(tf.zeros([no_hid_unit])),\n",
    "#         'output_bias_delta' : tf.Variable(tf.zeros([no_output_unit]))\n",
    "#     }\n",
    "\n",
    "\n",
    "#     ###### Forward Propagate ######\n",
    "#     # Hidden Layer\n",
    "#     input_to_hid_layer = tf.add(tf.matmul(tf_training_dataset, weights['input_to_hid_wghts']), biases['hid_bias'])\n",
    "#     hid_layer_state = tf.sigmoid(inputs_to_hid, name=None)\n",
    "\n",
    "#     # Output Layer\n",
    "#     hid_to_output_layer = tf.add(tf.matmul(hid_layer_state, weights['hid_to_output_wghts']), biases['output_bias'])\n",
    "#     output_layer_state = tf.nn.softmax(hid_to_output_layer, name=None)\n",
    "#     error_derivative = tf.sub(output_layer_state, tf_training_labels, name=None)  # -(y - y_hat) = (y_hat - y)\n",
    "#     # The above three lines of code can also be combined into one line as below.\n",
    "#     loss_CE = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(hid_to_output_layer, tf_training_labels))\n",
    "\n",
    "\n",
    "#     ####### Back Propagate #######\n",
    "#     # Output_Layer\n",
    "#     hid_to_output_wghts_gradient =  tf.matmul(tf.transpose(hid_layer_state), error_derivative)\n",
    "#     output_bias_gradient =  tf.reduce_sum(error_derivative, 0)  # sum all the row_value for each column \n",
    "\n",
    "#     # Hidden_Layer\n",
    "#     back_propagate_deriv = tf.mul(tf.transpose(tf.matmul(weights['hid_to_output_wghts'], tf.transpose(error_derivative))),\n",
    "#                                   tf.mul(hid_layer_state, 1.0-hid_layer_state)\n",
    "#                                  )\n",
    "#     input_to_hid_wghts_gradient = tf.matmul(tf.transpose(tf_training_dataset), back_propagate_deriv)\n",
    "#     hid_bias_gradient = tf.reduce_sum(back_propagate_deriv, 0)\n",
    "\n",
    "#     ####### Weight Update  #######\n",
    "#     # Update Weights\n",
    "#     weights['input_to_hid_wghts_delta'] = tf.mul(momentum, weights['input_to_hid_wghts_delta']) \\\n",
    "#                                 + tf.div(input_to_hid_wghts_gradient, batch_size)\n",
    "#     weights['input_to_hid_wghts'] = weights['input_to_hid_wghts'] - tf.mul(learning_rate,weights['input_to_hid_wghts_delta'])\n",
    "\n",
    "#     weights['hid_to_output_wghts_delta'] = tf.mul(momentum, weights['hid_to_output_wghts_delta']) \\\n",
    "#                                 + tf.div(hid_to_output_wghts_gradient, batch_size)\n",
    "#     weights['hid_to_output_wghts'] = weights['hid_to_output_wghts'] - tf.mul(learning_rate,weights['hid_to_output_wghts_delta'])\n",
    "\n",
    "\n",
    "#     ####### Bias Update  #######\n",
    "#     biases['hid_bias_delta'] = tf.mul(momentum, biases['hid_bias_delta']) + tf.div(hid_bias_gradient, batch_size)\n",
    "#     biases['hid_bias'] = biases['hid_bias'] - tf.mul(learning_rate,biases['hid_bias_delta'])\n",
    "\n",
    "#     biases['output_bias_delta'] = tf.mul(momentum, biases['output_bias_delta']) + tf.div(output_bias_gradient, batch_size)\n",
    "#     biases['output_bias'] = biases['output_bias'] - tf.mul(learning_rate,biases['output_bias_delta'])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# And then We can print and look at the data:\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(init)\n",
    "#     for i in range(2):\n",
    "#         print ('Forward Pass')\n",
    "#         print ('The input_to_hid_layer is: ', [w for no, w in enumerate(sess.run(input_to_hid_layer))])\n",
    "#         print ('')\n",
    "#         print ('The hid_layer_state is: ', [w for no, w in enumerate(sess.run(hid_layer_state))])\n",
    "#         print ('')\n",
    "#         print ('The hid_to_output_layer is: ', [w for no, w in enumerate(sess.run(hid_to_output_layer))])\n",
    "#         print ('')\n",
    "#         print ('The output_layer_state is: ', [w for no, w in enumerate(sess.run(output_layer_state))])\n",
    "#         print ('')\n",
    "#         print ('The error_derivative is: ', [w for no, w in enumerate(sess.run(error_derivative))])\n",
    "#         print ('')\n",
    "#         print ('The Loss value: ', sess.run(loss_CE))\n",
    "#         print ('')\n",
    "#         print ('')\n",
    "\n",
    "#         print ('Back Propagate')\n",
    "#         print ('The hid_to_output_wghts_gradient is: ', [w for no, w in enumerate(sess.run(hid_to_output_wghts_gradient))])\n",
    "#         print ('')\n",
    "#         print ('The hid_to_output_bias_gradient is: ', [w for no, w in enumerate(sess.run(hid_to_output_bias_gradient))])\n",
    "#         print ('')\n",
    "#         print ('The back_propagate_deriv is: ', [w for no, w in enumerate(sess.run(back_propagate_deriv))])\n",
    "#         print ('')\n",
    "#         print ('The input_to_hid_wghts_gradient is: ', [w for no, w in enumerate(sess.run(input_to_hid_wghts_gradient))])\n",
    "#         print ('')\n",
    "#         print ('')\n",
    "\n",
    "#         print ('Weight Update')\n",
    "#         print ('The input_to_hid_wghts_delta is: ', [w for no, w in enumerate(sess.run(weights['input_to_hid_wghts_delta']))])\n",
    "#         print ('')\n",
    "#         print ('The new input_to_hid_wghts is: ', [w for no, w in enumerate(sess.run(weights['input_to_hid_wghts']))])\n",
    "#         print ('')\n",
    "#         print ('The hid_to_output_wghts_delta is: ', [w for no, w in enumerate(sess.run(weights['hid_to_output_wghts_delta']))])\n",
    "#         print ('')\n",
    "#         print ('The new hid_to_output_wghts is: ', [w for no, w in enumerate(sess.run(weights['hid_to_output_wghts']))])\n",
    "#         print ('')\n",
    "#         print ('')\n",
    "\n",
    "#         print ('Bias Update')\n",
    "#         print ('The hid_bias_delta is: ', [w for no, w in enumerate(sess.run(biases['hid_bias_delta']))])\n",
    "#         print ('')\n",
    "#         print ('The new hid_bias is: ', [w for no, w in enumerate(sess.run(biases['hid_bias']))])\n",
    "#         print ('')\n",
    "#         print ('The output_bias_delta is: ', [w for no, w in enumerate(sess.run(biases['output_bias_delta']))])\n",
    "#         print ('')\n",
    "#         print ('The new output_bias is: ', [w for no, w in enumerate(sess.run(biases['output_bias']))])\n",
    "#         print ('')\n",
    "#         print ('')\n",
    "#         print ('')\n",
    "#         print ('')\n",
    "#         print ('')\n",
    "#         print ('')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
