{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Files\n",
    "#%pylab inline\n",
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display, Image\n",
    "from scipy import ndimage\n",
    "from six.moves import cPickle as pickle\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow as tf\n",
    "\n",
    "# Config the matplotlib backend as plotting inline in IPython\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/sam/All-Program/App-DataSet/Deep-Neural-Nets/Tensor-Flow-Learn/DataFolder/Training/a', '/Users/sam/All-Program/App-DataSet/Deep-Neural-Nets/Tensor-Flow-Learn/DataFolder/Training/b', '/Users/sam/All-Program/App-DataSet/Deep-Neural-Nets/Tensor-Flow-Learn/DataFolder/Training/c']\n",
      "\n",
      "/Users/sam/All-Program/App-DataSet/Deep-Neural-Nets/Tensor-Flow-Learn/DataFolder/Training/a\n",
      "Running for folder a\n",
      "Yes+Yes--The pickle file exists\n",
      "\n",
      "/Users/sam/All-Program/App-DataSet/Deep-Neural-Nets/Tensor-Flow-Learn/DataFolder/Training/b\n",
      "Running for folder b\n",
      "Yes+Yes--The pickle file exists\n",
      "\n",
      "/Users/sam/All-Program/App-DataSet/Deep-Neural-Nets/Tensor-Flow-Learn/DataFolder/Training/c\n",
      "Running for folder c\n",
      "Yes+Yes--The pickle file exists\n"
     ]
    }
   ],
   "source": [
    "# Training Dataset Builder: Get the pixels, prepross them store the dataset using pickle files into the disk\n",
    "\n",
    "image_size = 28\n",
    "\n",
    "def image_pixel_standarize(image_pixel_val):\n",
    "    return(image_pixel_val - 255.0/2)/255.0\n",
    "    \n",
    "\n",
    "def bld_pixels_arr(folder_name, min_no_of_image, max_no_of_image=None):\n",
    "    # Build the pickle file for one Alphabet Image\n",
    "    if max_no_of_image:\n",
    "        image_filenames = os.listdir(folder_name)[0:max_no_of_image]\n",
    "    else:\n",
    "        image_filenames = os.listdir(folder_name)\n",
    "\n",
    "    # Assign a numpy multudimensional array to store the image pixels, the image is 28*28 pixels\n",
    "    dataset = np.ndarray(shape=(len(image_filenames), \n",
    "                                image_size, \n",
    "                                image_size\n",
    "                               ),\n",
    "                         dtype=np.float32)\n",
    "    print (dataset.shape)\n",
    "    for num_images, image in enumerate(image_filenames):\n",
    "#         print (num_images)\n",
    "        image_file_dir = os.path.join(folder_name, image)\n",
    "        try:\n",
    "            image_pixels = ndimage.imread(image_file_dir).astype(float)\n",
    "            image_standarized = image_pixel_standarize(image_pixels)\n",
    "\n",
    "            if image_standarized.shape != (image_size, image_size):\n",
    "                raise Exception('Unexpected image shape: %s' % str(image_standarized.shape))\n",
    "            dataset[num_images, :, :] = image_standarized\n",
    "        except IOError as e:\n",
    "            print('Could not read:', image, ':', e, '- hence skipping.')\n",
    "        \n",
    "    dataset = dataset[0:num_images+1, :, :]  # Will combine all the image pixels for 1 type Example All the 'a'\n",
    "#     print (dataset.shape)\n",
    "#     print (dataset)\n",
    "    if num_images < min_no_of_image:\n",
    "        raise Exception('Many fewer images than expected: %d < %d' %(num_images, min_no_of_image))\n",
    "\n",
    "    print('Complete Training/Crossvalidation dataset :', dataset.shape)\n",
    "    print('Mean:', np.mean(dataset))\n",
    "    print('Standard deviation:', np.std(dataset))    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "        \n",
    "def crt_dataset(data_folders, dataset_type, min_no_of_image, max_no_of_image=None, force=None):    \n",
    "    dataset_dir_n_names = []\n",
    "    # One folder would comtain all the images pertaining to one particular alphabet.\n",
    "    for folder in data_folders:  # [0:2]  \n",
    "        print ('')\n",
    "        print (folder)\n",
    "        print ('Running for folder',os.path.basename(folder))\n",
    "        set_filename = dataset_type + os.path.basename(folder) + '.pickle'\n",
    "#         print (set_filename)\n",
    "        dataset_dir_n_names.append(set_filename)\n",
    "#         Ckech if the pickle file already exists, if not then we create one for each\n",
    "        if os.path.exists(set_filename) and not force:\n",
    "            print ('Yes+Yes--The pickle file exists')\n",
    "        else:\n",
    "            dataset = bld_pixels_arr(folder, min_no_of_image=min_no_of_image, max_no_of_image=max_no_of_image)\n",
    "            try:\n",
    "                with open(set_filename, 'wb') as f:\n",
    "                    pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)\n",
    "            except Exception as e:\n",
    "                print('Unable to save data to', set_filename, ':', e)\n",
    "    return dataset_dir_n_names\n",
    "                \n",
    "\n",
    "DataPath_train = \"/Users/sam/All-Program/App-DataSet/Deep-Neural-Nets/Tensor-Flow-Learn/DataFolder/Training/\"\n",
    "data_folders_train = [os.path.join(DataPath_train,folders) for folders in os.listdir(DataPath_train) if folders!='.DS_Store']    \n",
    "print (data_folders_train)\n",
    "\n",
    "training_dataset_dir = '/Users/sam/All-Program/App-DataSet/Deep-Neural-Nets/Tensor-Flow-Learn/DataPrepared/Training/'\n",
    "# test_dataset_dir = \"/Users/sam/All-Program/App-DataSet/Deep-Neural-Nets/Udacity/Datapreparation/test_dataset/\"\n",
    "\n",
    "training_data_names_n_dir = crt_dataset(data_folders=data_folders_train, \n",
    "                            dataset_type=training_dataset_dir, \n",
    "                            min_no_of_image=4, \n",
    "                            max_no_of_image=None, \n",
    "                            force=None)\n",
    "# test_data_names_n_dir = crt_dataset(data_folders=data_folders_test, \n",
    "#                             dataset_type=test_dataset_dir, \n",
    "#                             min_no_of_image=1800, \n",
    "#                             max_no_of_image=None, \n",
    "#                             force=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/sam/All-Program/App-DataSet/Deep-Neural-Nets/Tensor-Flow-Learn/DataPrepared/Training/a.pickle', '/Users/sam/All-Program/App-DataSet/Deep-Neural-Nets/Tensor-Flow-Learn/DataPrepared/Training/b.pickle', '/Users/sam/All-Program/App-DataSet/Deep-Neural-Nets/Tensor-Flow-Learn/DataPrepared/Training/c.pickle']\n",
      "training/teting size per class =  9\n",
      "crossvalid size per class =  3\n"
     ]
    }
   ],
   "source": [
    "# Create training and crossvalidation dataset \n",
    "def initialize_arrays(no_of_images, img_size):\n",
    "    dataset = np.ndarray((no_of_images, img_size, img_size), dtype=np.float32)\n",
    "    labels = np.ndarray(no_of_images, dtype=np.int32)\n",
    "    return dataset, labels\n",
    "\n",
    "def train_valid_test_builder(training_data_names_n_dir, training_size, crossvalid_size=0):\n",
    "    no_of_labels = len(training_data_names_n_dir)\n",
    "    training_dataset, training_labels = initialize_arrays(training_size, image_size)\n",
    "    if crossvalid_size!=0:\n",
    "        crossvalid_dataset, crossvalid_labels = initialize_arrays(crossvalid_size, image_size)\n",
    "    else:\n",
    "        crossvalid_dataset, crossvalid_labels = None, None\n",
    "\n",
    "    training_size_per_class = training_size // no_of_labels\n",
    "    crossvalid_size_per_class = crossvalid_size // no_of_labels\n",
    "    print ('training/teting size per class = ',training_size_per_class)\n",
    "    print ('crossvalid size per class = ', crossvalid_size_per_class)\n",
    "    \n",
    "    start_v, start_t = 0, 0\n",
    "    end_v, end_t = crossvalid_size_per_class, training_size_per_class\n",
    "    end_vt = crossvalid_size_per_class+training_size_per_class\n",
    "\n",
    "    for label, pickle_files in enumerate(training_data_names_n_dir):      \n",
    "#         print ('Running for alphabet: ', os.path.basename(pickle_files))\n",
    "        try:\n",
    "            with open(pickle_files, 'rb') as f:\n",
    "                letter_data = pickle.load(f)\n",
    "                np.random.shuffle(letter_data)          # This will suffle the data which will give the top elements as random\n",
    "                if crossvalid_dataset is not None:\n",
    "                    crossvalid_for_letter = letter_data[0:crossvalid_size_per_class, :, :]\n",
    "                    crossvalid_dataset[start_v:end_v, :, :] = crossvalid_for_letter\n",
    "                    crossvalid_labels[start_v:end_v] = label\n",
    "                    start_v += crossvalid_size_per_class\n",
    "                    end_v += crossvalid_size_per_class\n",
    "\n",
    "                training_for_letter = letter_data[crossvalid_size_per_class:end_vt, :, :]\n",
    "                training_dataset[start_t:end_t, :, :] = training_for_letter\n",
    "                training_labels[start_t:end_t] = label\n",
    "                start_t += training_size_per_class\n",
    "                end_t += training_size_per_class\n",
    "        except Exception as e:\n",
    "            print('Unable to process data from', pickle_files, ':', e)\n",
    "            raise\n",
    "    return crossvalid_dataset, crossvalid_labels, training_dataset, training_labels\n",
    "\n",
    "\n",
    "\n",
    "# Load the pixel Data Stored\n",
    "training_size = 27      # Total Train size including all different labels\n",
    "crossvalid_size = 9     # Total Crossvalid size including all different labels\n",
    "batch_size = 12\n",
    "\n",
    "print (training_data_names_n_dir)\n",
    "crossvalid_dataset, crossvalid_labels, training_dataset, training_labels = \\\n",
    "train_valid_test_builder(training_data_names_n_dir, training_size, crossvalid_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressed pickle size: 113426\n"
     ]
    }
   ],
   "source": [
    "complete_dataset_path = '/Users/sam/All-Program/App-DataSet/Deep-Neural-Nets/Tensor-Flow-Learn/DataPrepared/dataset_complete.p'\n",
    "\n",
    "try:\n",
    "    f = open(complete_dataset_path, 'wb')\n",
    "    dataset_complete = {\n",
    "        'training_dataset': training_dataset,\n",
    "        'training_labels': training_labels,\n",
    "        'crossvalid_dataset': crossvalid_dataset,\n",
    "        'crossvalid_labels': crossvalid_labels,\n",
    "#         'test_dataset': test_dataset,\n",
    "#         'test_labels': test_labels,\n",
    "    }\n",
    "    pickle.dump(dataset_complete, f, pickle.HIGHEST_PROTOCOL)\n",
    "    f.close()\n",
    "except Exception as e:\n",
    "    print('Unable to save data to', complete_dataset_path, ':', e)\n",
    "    raise\n",
    "\n",
    "    \n",
    "statinfo = os.stat(complete_dataset_path)\n",
    "print('Compressed pickle size:', statinfo.st_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
