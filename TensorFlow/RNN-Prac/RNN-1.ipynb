{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading CSV file...\n",
      "Parsed 79170 sentences.\n",
      "Found 65752 unique words tokens.\n",
      "Using vocabulary size 8000.\n",
      "The least frequent word in our vocabulary is 'pie' and appeared 10 times.\n",
      "\n",
      "Example sentence: 'SENTENCE_START i joined a new league this year and they have different scoring rules than i'm used to. SENTENCE_END'\n",
      "\n",
      "Example sentence after Pre-processing: '['SENTENCE_START', 'i', 'joined', 'a', 'new', 'league', 'this', 'year', 'and', 'they', 'have', 'different', 'scoring', 'rules', 'than', 'i', \"'m\", 'used', 'to', '.', 'SENTENCE_END']'\n",
      "Compressed pickle size: 8233872\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import itertools\n",
    "import numpy as np\n",
    "import csv\n",
    "import pickle\n",
    "import os\n",
    "# sentence_start_token = 'start'\n",
    "# sentence_end_token = 'end'\n",
    "# unknown_token = 'unknown'\n",
    "# text = 'Oh! my god, I dont know why did I buy this product, A waste of momney. This is outrageous, I dont get it. \\\n",
    "# My name is Sam Helsen and I like trekking. What is your name? Are you Sam Helsen?. I am going to America, the land of free.\\\n",
    "#  Last night I saw a bad man. Sam Helsen was caught smoking weed amid the crowd, this is indeed an outrageous attempt. The police charged fine, such a poor fellow. \\\n",
    "#  Sam has lost a lot of money in gambling and now he is loosing by paying fines. Sam bought a product yesterday, which was infact \\\n",
    "#  an expire product. He applied it yesterday to his face and got allergic itching. Such a unlucky fellow. He ones agin lost a lot of money \\\n",
    "#  treating the ailment. '   \n",
    "data_path = \"/Users/sam/All-Program/App-DataSet/Deep-Neural-Nets/Word-Search-NNets/reddit-comments-2015-08.csv\"\n",
    "\n",
    "vocab_size = 8000\n",
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "sentence_start_token = \"SENTENCE_START\"\n",
    "sentence_end_token = \"SENTENCE_END\"\n",
    " \n",
    "# Read the data and append SENTENCE_START and SENTENCE_END tokens\n",
    "print (\"Reading CSV file...\")\n",
    "with open(data_path, 'r') as f:\n",
    "    reader = csv.reader(f, skipinitialspace=True)\n",
    "    next(reader)\n",
    "    # Split full comments into sentences\n",
    "    sentences = itertools.chain(*[nltk.sent_tokenize(x[0].lower()) for x in reader])\n",
    "    # Append SENTENCE_START and SENTENCE_END\n",
    "    sentences = [\"%s %s %s\" % (sentence_start_token, x, sentence_end_token) for x in sentences]\n",
    "print (\"Parsed %d sentences.\" % (len(sentences)))\n",
    "     \n",
    "\n",
    "# Tokenize the sentences into words\n",
    "tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    " \n",
    "# Count the word frequencies\n",
    "word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
    "print (\"Found %d unique words tokens.\" % len(word_freq.items()))\n",
    " \n",
    "# Get the most common words and build index_to_word and word_to_index vectors\n",
    "vocab = word_freq.most_common(vocab_size-1)\n",
    "index_to_word = [x[0] for x in vocab]\n",
    "index_to_word.append(unknown_token)\n",
    "word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)])\n",
    " \n",
    "print (\"Using vocabulary size %d.\" % vocab_size)\n",
    "print (\"The least frequent word in our vocabulary is '%s' and appeared %d times.\" % (vocab[-1][0], vocab[-1][1]))\n",
    " \n",
    "# Replace all words not in our vocabulary with the unknown token\n",
    "for i, sent in enumerate(tokenized_sentences):\n",
    "    tokenized_sentences[i] = [w if w in word_to_index else unknown_token for w in sent]\n",
    "    \n",
    "print (\"\\nExample sentence: '%s'\" % sentences[0])\n",
    "print (\"\\nExample sentence after Pre-processing: '%s'\" % tokenized_sentences[0])\n",
    " \n",
    "# Create the training data\n",
    "X_train = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in tokenized_sentences])\n",
    "y_train = np.asarray([[word_to_index[w] for w in sent[1:]] for sent in tokenized_sentences])\n",
    "\n",
    "\n",
    "cleaned_dataset_path = '/Users/sam/All-Program/App-DataSet/Deep-Neural-Nets/Word-Search-NNets/reddit-comments-2015-08.p'\n",
    "\n",
    "try:\n",
    "    f = open(cleaned_dataset_path, 'wb')\n",
    "    dataset_cleaned = {\n",
    "        'X_train': X_train,\n",
    "        'y_train': y_train\n",
    "    }\n",
    "    pickle.dump(dataset_cleaned, f, pickle.HIGHEST_PROTOCOL)\n",
    "    f.close()\n",
    "except Exception as e:\n",
    "    print('Unable to save data to', cleaned_dataset_path, ':', e)\n",
    "    raise\n",
    "\n",
    "    \n",
    "statinfo = os.stat(cleaned_dataset_path)\n",
    "print('Compressed pickle size:', statinfo.st_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0111803398875\n",
      "\n",
      "The shape of U weight matrix is:  (8000, 100)\n",
      "The shape of W weight matrix is:  (100, 100)\n",
      "The shape of V weight matrix is:  (100, 8000)\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 8000\n",
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "sentence_start_token = \"START\"\n",
    "sentence_end_token = \"END\"\n",
    "\n",
    "with open(cleaned_dataset_path, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    X_train = (data['X_train'])\n",
    "    y_train = (data['y_train'])\n",
    "    \n",
    "# Assign instance variables\n",
    "no_output_units = vocab_size\n",
    "no_input_units = vocab_size\n",
    "no_hidden_units = 100\n",
    "# Randomly initialize the network parameters, \n",
    "# We initialize the wights as such to avoid vanishing weights and weights explosion\n",
    "U = np.random.uniform(-np.sqrt(1./no_input_units), np.sqrt(1./no_input_units), (no_input_units, no_hidden_units))\n",
    "W = np.random.uniform(-np.sqrt(1./no_hidden_units), np.sqrt(1./no_hidden_units), (no_hidden_units, no_hidden_units))\n",
    "V = np.random.uniform(-np.sqrt(1./no_hidden_units), np.sqrt(1./no_hidden_units), (no_hidden_units, no_output_units))\n",
    "    \n",
    "\n",
    "print (-np.sqrt(1./word_dim))\n",
    "print ('')\n",
    "print ('The shape of U weight matrix is: ', U.shape)\n",
    "print ('The shape of W weight matrix is: ', W.shape)\n",
    "print ('The shape of V weight matrix is: ', V.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The no of words in x is:  45\n",
      "(5, 100) (4, 8000)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "\n",
    "# The hidden state would be in the dimension of [no_of_sequence x no_hidden_units]\n",
    "def fprop(x):\n",
    "    # The total number of time steps\n",
    "    no_of_sequence = len(x)\n",
    "    # During forward propagation we save all hidden states in s because need them later.\n",
    "    # We add one additional element for the initial hidden, which we set to 0\n",
    "    hidden_layer_state = np.zeros((no_of_sequence + 1, no_hidden_units))\n",
    "    # We just define s[-1] as a zero, vector. because we look back one sequence back while computig the inputs_to_hid_layer at time t=0\n",
    "    hidden_layer_state[-1] = np.zeros(hidden_dim)\n",
    "    output_layer_state = np.zeros((no_of_sequence, no_output_units))\n",
    "\n",
    "    for t in np.arange(no_of_sequence):\n",
    "        # Note that we are indxing U by x[t]. This is the same as multiplying U with a one-hot vector.\n",
    "        inputs_to_hid_layer = U[x[t],:] + W.dot(hidden_layer_state[t-1]) \n",
    "        # Here U[x[t],:] would just output the correcsponding row from U. This is equivallend of taking the dot\n",
    "        # product of the U matrix with the one hot vector for the corresponding word.\n",
    "        hidden_layer_state[t] = np.tanh(inputs_to_hid_layer)\n",
    "        #print (hidden_layer_state[t].shape)\n",
    "        \n",
    "        imputs_to_output_layer = (hidden_layer_state[t].T).dot(V)\n",
    "        output_layer_state[t] =  softmax(imputs_to_output_layer)\n",
    "        #print (output_layer_state[t].shape)\n",
    "    return [hidden_layer_state, output_layer_state]\n",
    "\n",
    "\n",
    "# Test for One training sample\n",
    "x = X_train[10]\n",
    "print ('The no of words in x is: ', len(x))\n",
    "hidden_layer_state, output_layer_state = fprop(x[0:4])\n",
    "print (hidden_layer_state.shape, output_layer_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual loss for 2 dataset: 8.98386077269\n",
      "Expected Loss for random predictions: 8.987197\n",
      "Average loss for 1000 dataset 8.98729075289\n"
     ]
    }
   ],
   "source": [
    "# Calculating the LOSS usinf Cross-Entropy\n",
    "\n",
    "# Remember X_train is an array of word indices and so is the y\n",
    "def cal_total_loss_CE(x, y):  # Using cross-entropy to calculate the total loss.\n",
    "    CE_tot = 0\n",
    "    for datarow_arr, label_arr in zip(x, y):\n",
    "        # datarow_arr is such a array of sequence of word indices of a sentence.\n",
    "        # Example: datarow = [0,4,7] then label = [4,7,1], where 0 indicates \"start\" and 1 indicates 'end'\n",
    "        hidden_layer_state, output_layer_state = fprop(datarow_arr)\n",
    "        # The above code will find the output_layer_state for that 1 sentence (datarow_arr).\n",
    "        prediction = output_layer_state[np.arange(len(label_arr)), label_arr]\n",
    "\n",
    "        CE_tot += -1 * np.sum(np.log(prediction))\n",
    "        # For a cross entropy we would want to get the dot product of the one_hot matrix formed by the actuls labels\n",
    "        # and the log(output_layer_state) but that would result in creating a very large one-hot matrix of vocab_size 8000.\n",
    "        # Finding the values in the columns of output_layer_state for indices in label_arr is the equivallent of doing the dot product\n",
    "    return CE_tot\n",
    " \n",
    "def calculate_avg_loss(x, y):\n",
    "    # Divide the total loss by the number of training examples\n",
    "    N = np.sum((len(y_i) for y_i in y))\n",
    "    return cal_total_loss_CE(x,y)/N\n",
    " \n",
    "    \n",
    "# For 2 examples\n",
    "print (\"Actual loss for %d dataset:\"%2,  calculate_avg_loss([X_train[10][0:3], X_train[11][0:3]], [y_train[10][0:3], y_train[11][0:3]]))\n",
    "\n",
    "# For the first 1000 sentences or dataset\n",
    "print (\"Expected Loss for random predictions: %f\" % np.log(vocabulary_size))\n",
    "print (\"Average loss for %d dataset\" %1000, calculate_avg_loss(X_train[:1000], y_train[:1000]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Loss for random predictions: 8.987197\n",
      "Actual loss: 8.987314\n"
     ]
    }
   ],
   "source": [
    "bptt_truncate = 3\n",
    "\n",
    "def bptt(x, y):\n",
    "    T = len(y)\n",
    "    # Perform forward propagation\n",
    "    o, s = forward_propagation(x)\n",
    "    print (o.shape, s.shape)\n",
    "    # We accumulate the gradients in these variables\n",
    "    dLdU = np.zeros(U.shape)\n",
    "    dLdV = np.zeros(V.shape)\n",
    "    dLdW = np.zeros(W.shape)\n",
    "    delta_o = o\n",
    "    delta_o[np.arange(len(y)), y] -= 1.\n",
    "    print (delta_o.shape)\n",
    "    # For each output backwards...\n",
    "    # Backpropogation works from right to left, so we reverse the arrat, ex [0,1,2,3] be becomes [3,2,1,0]\n",
    "    for t in np.arange(T)[::-1]:\n",
    "        print ('')\n",
    "        print ('aaaaaaaaaaaaaaaaa ', t)\n",
    "        dLdV += np.outer(delta_o[t], s[t].T)\n",
    "#         print ('s[t].T.shape ', s[t].T.shape)\n",
    "#         print ('dLdV.shape ',dLdV.shape)\n",
    "#         print ('V.T.shape ', V.T.shape)\n",
    "#         print ('delta_o[t].shape ', delta_o[t].shape)\n",
    "#         print ('')\n",
    "        \n",
    "#         print ('at t= %d s(t) is : '%t, 1 - (s[t] ** 2))\n",
    "#         print ('at t= %d delta_o[t] is: '%t, delta_o[t])\n",
    "#         print (delta_o[t]) * (1 - (s[t] ** 2))\n",
    "#         delta_t = V.T.dot(delta_o[t]) * (1 - (s[t] ** 2))\n",
    "#         print (delta_t)\n",
    "#         print ('t-bptt_truncate is: ', t-bptt_truncate)\n",
    "#         print ('max(0, t-bptt_truncate) is: ',max(0, t-bptt_truncate))\n",
    "#         print ('np.arange(max(0, t-self.bptt_truncate), t+1) ', np.arange(max(0, t-bptt_truncate), t+1))\n",
    "#         print ('np.arange(max(0, t-self.bptt_truncate), t+1)[::-1] ', np.arange(max(0, t-bptt_truncate), t+1)[::-1])\n",
    "#         #         print (np.arange(max(0, t-bptt_truncate), t+1)[::-1])\n",
    "#         for bptt_step in np.arange(max(0, t-bptt_truncate), t+1)[::-1]:\n",
    "#             print ('popopopopoo ', bptt_step)\n",
    "#             print (s[bptt_step-1])\n",
    "\n",
    "        # Initial delta calculation\n",
    "        delta_t = V.T.dot(delta_o[t]) * (1 - (s[t] ** 2))\n",
    "        # Backpropagation through time (for at most self.bptt_truncate steps)\n",
    "        for bptt_step in np.arange(max(0, t-bptt_truncate), t+1)[::-1]:\n",
    "            print ('bbbbbbbbbbbbbbbbbb ', bptt_step)\n",
    "            # print \"Backpropagation step t=%d bptt step=%d \" % (t, bptt_step)\n",
    "            dLdW += np.outer(delta_t, s[bptt_step-1])  \n",
    "            print ('s[bptt_step-1]', s[bptt_step-1])\n",
    "            print ('dLdW',  dLdW)\n",
    "#             dLdU[:,x[bptt_step]] += delta_t\n",
    "            # Update delta for next step\n",
    "            delta_t = W.T.dot(delta_t) * (1 - s[bptt_step-1] ** 2)\n",
    "            print ('delta_t ', delta_t)\n",
    "        break\n",
    "#     return [dLdU, dLdV, dLdW]\n",
    "\n",
    "# We take a small example to \n",
    "\n",
    "bptt([0,1,2], [1,2,3]) # 0,1,2,3,4 is the word sequence for example:  0->I, 1->am, 2->going, 3->to, 4->hit, 5-> you "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 8000) (4, 100)\n",
      "(3, 8000)\n",
      "\n",
      "aaaaaaaaaaaaaaaaa  2\n",
      "bbbbbbbbbbbbbbbbbb  2\n",
      "s[bptt_step-1] [-0.00282782 -0.00907338 -0.01465256  0.00252337 -0.00382819  0.01635668\n",
      "  0.01294681 -0.0087975   0.00392096 -0.00873958  0.00309464 -0.00527214\n",
      " -0.01122418  0.00481832 -0.00731619 -0.00896967 -0.01245067  0.00612713\n",
      "  0.00305838  0.00427345 -0.0033523  -0.00328978  0.01520627 -0.00840482\n",
      "  0.00568465  0.01520935  0.00343978 -0.00189767 -0.0042385   0.00297322\n",
      " -0.00317739 -0.01137146  0.00101308 -0.00427126  0.01046035 -0.00404881\n",
      "  0.00497765 -0.00338904  0.00126298  0.00519548  0.00594546 -0.01020443\n",
      "  0.00113324 -0.00168416 -0.00573876  0.00343812  0.0019591   0.0009989\n",
      "  0.00307361 -0.00879284  0.0086315   0.01064532  0.00254845  0.00365477\n",
      "  0.00803679  0.00463294 -0.01071622 -0.00732892 -0.00702249 -0.00749406\n",
      "  0.00489553 -0.00900501 -0.00458312  0.0084647  -0.0129267  -0.0016163\n",
      " -0.01029885  0.01285934  0.00813531 -0.00061622 -0.01112088 -0.00611581\n",
      " -0.00277355 -0.00056213 -0.00795453  0.00856676  0.0108411   0.00336074\n",
      " -0.00071098  0.00289204  0.00742276  0.0071902   0.00913908 -0.00071426\n",
      " -0.00253899  0.00168883  0.00025476  0.00162674  0.00761646 -0.00104957\n",
      " -0.00047864 -0.00334336  0.00674752 -0.00098043  0.00739256 -0.00546332\n",
      "  0.00843859 -0.00600402 -0.01235788 -0.00338015]\n",
      "dLdW [[  5.45345356e-05   1.74980407e-04   2.82575240e-04 ...,   1.15787714e-04\n",
      "    2.38322179e-04   6.51862950e-05]\n",
      " [ -2.53699190e-04  -8.14023389e-04  -1.31456349e-03 ...,  -5.38654063e-04\n",
      "   -1.10869457e-03  -3.03252060e-04]\n",
      " [ -2.06265031e-04  -6.61825366e-04  -1.06877944e-03 ...,  -4.37941867e-04\n",
      "   -9.01401856e-04  -2.46552997e-04]\n",
      " ..., \n",
      " [  1.03453847e-04   3.31943712e-04   5.36054727e-04 ...,   2.19653184e-04\n",
      "    4.52105183e-04   1.23660593e-04]\n",
      " [  1.58112822e-04   5.07323396e-04   8.19274758e-04 ...,   3.35705107e-04\n",
      "    6.90971175e-04   1.88995632e-04]\n",
      " [  1.01556630e-04   3.25856270e-04   5.26224137e-04 ...,   2.15625013e-04\n",
      "    4.43814126e-04   1.21392808e-04]]\n",
      "delta_t  [ 0.03613235 -0.01004392 -0.0277446  -0.00484723  0.00756317 -0.0003946\n",
      "  0.03010735  0.00560675 -0.01858176 -0.02095928  0.02373406 -0.02617594\n",
      "  0.03871654  0.00171607 -0.0170462   0.03365793 -0.01572382 -0.06439832\n",
      "  0.04213392  0.02027871  0.02546765  0.00835897 -0.04323242 -0.06019908\n",
      " -0.01420456 -0.05919622  0.01498772  0.029294    0.00807217 -0.01167355\n",
      "  0.00819032 -0.07169576 -0.03352904  0.01519357 -0.00852213  0.05655032\n",
      "  0.02600661 -0.0315617   0.05105382 -0.00932806  0.00264101  0.00030413\n",
      " -0.00010632  0.0207328  -0.01069807  0.05174752  0.03717869  0.00568287\n",
      "  0.08557233 -0.01368089 -0.03954056  0.02005992 -0.00368382  0.03903766\n",
      "  0.05068884  0.00913284 -0.00794913 -0.01854072  0.0072686   0.04680465\n",
      "  0.01240603 -0.07376298 -0.0103674  -0.00526099 -0.00221943 -0.03620808\n",
      "  0.03545287  0.04492878  0.01885405  0.02001389 -0.04477353  0.0139731\n",
      " -0.06533497  0.00498822 -0.00504294 -0.00711475  0.04764721  0.03440654\n",
      "  0.03955171  0.04632952  0.00670002 -0.04118855  0.01885985  0.01804233\n",
      " -0.00593722  0.03600096 -0.05465175 -0.02484103 -0.0101667  -0.03219717\n",
      " -0.01451878  0.03540522 -0.00664891 -0.04869444  0.03733366  0.01145899\n",
      " -0.04777438 -0.00631556 -0.00333962  0.02207343]\n",
      "bbbbbbbbbbbbbbbbbb  1\n",
      "s[bptt_step-1] [ -6.72094411e-03   9.14525536e-03   5.25473048e-03   1.01537120e-02\n",
      "   6.25010892e-03  -1.08332230e-03   5.89865750e-03   5.72450267e-03\n",
      "   3.98038533e-03  -8.88237837e-03   2.50818454e-03  -7.96396165e-04\n",
      "  -4.39648827e-03   6.55643935e-03  -2.28301266e-03  -2.75261517e-04\n",
      "  -3.62612030e-03   5.58001983e-03   2.34862294e-03  -1.04198684e-02\n",
      "  -5.30607212e-03   9.74529782e-03   7.75670763e-03  -5.97502630e-03\n",
      "  -6.28956697e-03  -9.62917464e-03  -9.52403263e-03   1.11732844e-03\n",
      "   5.86649337e-03   5.29720076e-03   8.93546297e-03  -4.36671097e-05\n",
      "  -1.10100554e-02   9.27349584e-03  -2.22482053e-03  -4.43555676e-03\n",
      "   6.57825596e-03   6.12271269e-03  -2.96975326e-03   9.21574746e-04\n",
      "   6.81617287e-03  -1.02176462e-02   4.74732417e-03   3.69297590e-03\n",
      "  -3.81218957e-03  -1.03154079e-02  -3.38062589e-03  -1.79122101e-03\n",
      "  -9.16424752e-03   5.75809347e-03   8.92691224e-03   7.22183617e-03\n",
      "   1.04191351e-02  -7.51003031e-03   1.27062691e-03   1.00860480e-02\n",
      "  -9.96738980e-03   6.55742737e-05  -8.74845219e-03   6.13079817e-03\n",
      "   5.75602101e-04   8.57272098e-03  -2.12000055e-03   1.04233586e-02\n",
      "   6.16725849e-03   6.28597073e-03  -2.32404177e-03  -4.44403414e-03\n",
      "   9.97191774e-03   8.39896986e-03   7.02936267e-03   1.03427972e-02\n",
      "   7.02362345e-03   9.82731997e-03  -9.99335177e-03   3.47286472e-03\n",
      "   7.95838261e-04  -3.20119012e-03  -8.00599541e-04  -4.38223200e-03\n",
      "  -7.74693710e-03  -1.00024971e-02   2.80217461e-04  -5.15197917e-03\n",
      "  -6.48126237e-03   7.44736117e-03   3.21546438e-04  -9.95411148e-03\n",
      "   4.33379182e-03   4.17835226e-03  -8.92944161e-04  -7.82608976e-03\n",
      "  -5.27798623e-03  -1.70731999e-03   1.09579762e-02  -7.47461564e-03\n",
      "  -3.83801300e-03   1.72698034e-03  -7.56272252e-03   1.06485028e-03]\n",
      "dLdW [[ -1.88308957e-04   5.05419957e-04   4.72440991e-04 ...,   1.78187568e-04\n",
      "   -3.49367439e-05   1.03661836e-04]\n",
      " [ -1.86194561e-04  -9.05877608e-04  -1.36734159e-03 ...,  -5.55999717e-04\n",
      "   -1.03273519e-03  -3.13947332e-04]\n",
      " [ -1.97951494e-05  -9.15556785e-04  -1.21456982e-03 ...,  -4.85856240e-04\n",
      "   -6.91577172e-04  -2.76096838e-04]\n",
      " ..., \n",
      " [  1.45900365e-04   2.74186314e-04   5.02868167e-04 ...,   2.08746338e-04\n",
      "    4.99868002e-04   1.16935468e-04]\n",
      " [  1.80558254e-04   4.76781674e-04   8.01725930e-04 ...,   3.29937641e-04\n",
      "    7.16227831e-04   1.85439432e-04]\n",
      " [ -4.67976485e-05   5.27723408e-04   6.42214054e-04 ...,   2.53745390e-04\n",
      "    2.76878912e-04   1.44897705e-04]]\n",
      "delta_t  [ 0.00423961  0.03205086  0.00827956  0.02457203  0.00131546 -0.0067454\n",
      " -0.00037276  0.0139266   0.00255794 -0.00264303  0.00784845  0.01569315\n",
      " -0.00123784 -0.02583778  0.01095713  0.01049187 -0.01369513 -0.01869454\n",
      " -0.00241293  0.0013722  -0.01497164  0.00547499  0.00318076  0.01396873\n",
      "  0.01780782  0.02728147 -0.01324194 -0.01128593  0.01003719 -0.0096338\n",
      "  0.01205339  0.02341695  0.01674687 -0.0142027   0.04275127 -0.00520977\n",
      "  0.00563443 -0.02723312 -0.01366933 -0.0023322   0.02685566 -0.02853536\n",
      " -0.00896526  0.02135165 -0.02123226  0.01210857  0.01424156 -0.01669032\n",
      " -0.04456015 -0.05171791 -0.00918057  0.02556424 -0.01222882 -0.00743325\n",
      "  0.02870178  0.02223406 -0.00833961  0.02402014 -0.00064177 -0.00019336\n",
      "  0.00683803  0.00689242 -0.01638376  0.00545002  0.00259126  0.01289186\n",
      " -0.0123545  -0.02633971  0.02857089  0.01320492  0.03744654  0.00399589\n",
      "  0.00331217 -0.00873095  0.01982314  0.02315258  0.03173838 -0.00421463\n",
      " -0.00538756 -0.00346311  0.00096482  0.01782389 -0.03915023  0.0174939\n",
      " -0.01310999  0.01733352  0.0199411  -0.02523595 -0.01423034 -0.01450359\n",
      "  0.01679212 -0.00726566 -0.03075166 -0.01178591 -0.01511208  0.00400077\n",
      "  0.02374045  0.00538833  0.01979783 -0.03683051]\n",
      "bbbbbbbbbbbbbbbbbb  0\n",
      "s[bptt_step-1] [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "dLdW [[ -1.88308957e-04   5.05419957e-04   4.72440991e-04 ...,   1.78187568e-04\n",
      "   -3.49367439e-05   1.03661836e-04]\n",
      " [ -1.86194561e-04  -9.05877608e-04  -1.36734159e-03 ...,  -5.55999717e-04\n",
      "   -1.03273519e-03  -3.13947332e-04]\n",
      " [ -1.97951494e-05  -9.15556785e-04  -1.21456982e-03 ...,  -4.85856240e-04\n",
      "   -6.91577172e-04  -2.76096838e-04]\n",
      " ..., \n",
      " [  1.45900365e-04   2.74186314e-04   5.02868167e-04 ...,   2.08746338e-04\n",
      "    4.99868002e-04   1.16935468e-04]\n",
      " [  1.80558254e-04   4.76781674e-04   8.01725930e-04 ...,   3.29937641e-04\n",
      "    7.16227831e-04   1.85439432e-04]\n",
      " [ -4.67976485e-05   5.27723408e-04   6.42214054e-04 ...,   2.53745390e-04\n",
      "    2.76878912e-04   1.44897705e-04]]\n",
      "delta_t  [ -1.60027828e-03  -1.19308272e-02  -2.13197795e-02   4.81735711e-03\n",
      "  -6.39037191e-03   1.67410648e-02   1.30388444e-02   7.84426098e-03\n",
      "  -4.48435214e-03  -2.94835877e-03   1.61192857e-02   1.53241429e-02\n",
      "   8.23467323e-04  -8.59297720e-03  -1.18645159e-02   7.68607004e-03\n",
      "  -5.40097127e-03  -7.55811098e-03   2.20728074e-02   2.96756975e-03\n",
      "   6.76226857e-03   1.17373232e-02   1.38148623e-02  -6.94926897e-03\n",
      "   2.53111623e-03  -1.05488918e-02  -4.62730259e-03  -1.45182668e-02\n",
      "  -8.91451016e-03   8.23434816e-03   1.92555482e-02  -5.87556806e-03\n",
      "   5.29209206e-03  -1.31648254e-02  -6.25414521e-03  -2.90128887e-03\n",
      "  -1.41552614e-02  -3.48669345e-03  -1.13769879e-02  -3.72440377e-03\n",
      "   2.21653802e-03   1.97551581e-03   8.32753012e-03   1.34703130e-03\n",
      "  -1.34501942e-03   1.64995600e-02  -5.30338115e-03  -3.11438081e-03\n",
      "  -2.85221099e-03   1.23104386e-03   1.33455389e-02  -4.59884899e-03\n",
      "  -1.86221047e-03  -7.78265107e-03  -4.95827162e-03  -9.55284446e-05\n",
      "   1.75398781e-03  -1.88136745e-02   7.96029210e-03  -5.71050988e-03\n",
      "  -2.35244337e-03  -5.17276305e-03   7.09746952e-03   2.68253731e-02\n",
      "   7.09988583e-04  -3.04247157e-03   1.52526666e-02   6.57637425e-03\n",
      "  -2.43503927e-02  -2.32083509e-04  -8.98280309e-03  -3.60938100e-03\n",
      "  -2.54781859e-03   1.55322843e-02   1.15492634e-02  -1.40268901e-04\n",
      "  -1.55134190e-02   6.47369197e-03  -3.52914785e-03  -1.13406986e-02\n",
      "   6.06790177e-04  -1.59870416e-02  -1.89604321e-02   5.89114575e-03\n",
      "  -5.66149805e-03   2.47511074e-02   2.13254593e-02  -2.14996657e-02\n",
      "  -6.27922028e-03  -2.02746952e-03   6.88201702e-03   6.50651782e-03\n",
      "  -1.64050238e-03  -6.67946044e-03   1.35819667e-02   7.48462458e-03\n",
      "   3.21943042e-03   9.77983019e-03  -5.28728420e-03  -5.75995065e-03]\n"
     ]
    }
   ],
   "source": [
    "bptt_truncate = 3\n",
    "\n",
    "def bptt(x, y):\n",
    "    T = len(y)\n",
    "    # Perform forward propagation\n",
    "    o, s = forward_propagation(x)\n",
    "    print (o.shape, s.shape)\n",
    "    # We accumulate the gradients in these variables\n",
    "    dLdU = np.zeros(U.shape)\n",
    "    dLdV = np.zeros(V.shape)\n",
    "    dLdW = np.zeros(W.shape)\n",
    "    delta_o = o\n",
    "    delta_o[np.arange(len(y)), y] -= 1.\n",
    "    print (delta_o.shape)\n",
    "    # For each output backwards...\n",
    "    # Backpropogation works from right to left, so we reverse the arrat, ex [0,1,2,3] be becomes [3,2,1,0]\n",
    "    for t in np.arange(T)[::-1]:\n",
    "        print ('')\n",
    "        print ('aaaaaaaaaaaaaaaaa ', t)\n",
    "        dLdV += np.outer(delta_o[t], s[t].T)\n",
    "#         print ('s[t].T.shape ', s[t].T.shape)\n",
    "#         print ('dLdV.shape ',dLdV.shape)\n",
    "#         print ('V.T.shape ', V.T.shape)\n",
    "#         print ('delta_o[t].shape ', delta_o[t].shape)\n",
    "#         print ('')\n",
    "        \n",
    "#         print ('at t= %d s(t) is : '%t, 1 - (s[t] ** 2))\n",
    "#         print ('at t= %d delta_o[t] is: '%t, delta_o[t])\n",
    "#         print (delta_o[t]) * (1 - (s[t] ** 2))\n",
    "#         delta_t = V.T.dot(delta_o[t]) * (1 - (s[t] ** 2))\n",
    "#         print (delta_t)\n",
    "#         print ('t-bptt_truncate is: ', t-bptt_truncate)\n",
    "#         print ('max(0, t-bptt_truncate) is: ',max(0, t-bptt_truncate))\n",
    "#         print ('np.arange(max(0, t-self.bptt_truncate), t+1) ', np.arange(max(0, t-bptt_truncate), t+1))\n",
    "#         print ('np.arange(max(0, t-self.bptt_truncate), t+1)[::-1] ', np.arange(max(0, t-bptt_truncate), t+1)[::-1])\n",
    "#         #         print (np.arange(max(0, t-bptt_truncate), t+1)[::-1])\n",
    "#         for bptt_step in np.arange(max(0, t-bptt_truncate), t+1)[::-1]:\n",
    "#             print ('popopopopoo ', bptt_step)\n",
    "#             print (s[bptt_step-1])\n",
    "\n",
    "        # Initial delta calculation\n",
    "        delta_t = V.T.dot(delta_o[t]) * (1 - (s[t] ** 2))\n",
    "        # Backpropagation through time (for at most self.bptt_truncate steps)\n",
    "        for bptt_step in np.arange(max(0, t-bptt_truncate), t+1)[::-1]:\n",
    "            print ('bbbbbbbbbbbbbbbbbb ', bptt_step)\n",
    "            # print \"Backpropagation step t=%d bptt step=%d \" % (t, bptt_step)\n",
    "            dLdW += np.outer(delta_t, s[bptt_step-1])  \n",
    "            print ('s[bptt_step-1]', s[bptt_step-1])\n",
    "            print ('dLdW',  dLdW)\n",
    "#             dLdU[:,x[bptt_step]] += delta_t\n",
    "            # Update delta for next step\n",
    "            delta_t = W.T.dot(delta_t) * (1 - s[bptt_step-1] ** 2)\n",
    "            print ('delta_t ', delta_t)\n",
    "        break\n",
    "#     return [dLdU, dLdV, dLdW]\n",
    "\n",
    "# We take a small example to \n",
    "\n",
    "bptt([0,1,2], [1,2,3]) # 0,1,2,3,4 is the word sequence for example:  0->I, 1->am, 2->going, 3->to, 4->hit, 5-> you "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,) (3,)\n",
      "[[10 11 12]\n",
      " [20 22 24]\n",
      " [30 33 36]\n",
      " [40 44 48]]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([1,2,3,4])\n",
    "b = np.array([10,11,12])\n",
    "print (a.shape, b.shape)\n",
    "print (np.outer(a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_check(self, x, y, h=0.001, error_threshold=0.01):\n",
    "    # Calculate the gradients using backpropagation. We want to checker if these are correct.\n",
    "    bptt_gradients = self.bptt(x, y)\n",
    "    # List of all parameters we want to check.\n",
    "    model_parameters = ['U', 'V', 'W']\n",
    "    # Gradient check for each parameter\n",
    "    for pidx, pname in enumerate(model_parameters):\n",
    "        # Get the actual parameter value from the mode, e.g. model.W\n",
    "        parameter = operator.attrgetter(pname)(self)\n",
    "        print \"Performing gradient check for parameter %s with size %d.\" % (pname, np.prod(parameter.shape))\n",
    "        # Iterate over each element of the parameter matrix, e.g. (0,0), (0,1), ...\n",
    "        it = np.nditer(parameter, flags=['multi_index'], op_flags=['readwrite'])\n",
    "        while not it.finished:\n",
    "            ix = it.multi_index\n",
    "            # Save the original value so we can reset it later\n",
    "            original_value = parameter[ix]\n",
    "            # Estimate the gradient using (f(x+h) - f(x-h))/(2*h)\n",
    "            parameter[ix] = original_value + h\n",
    "            gradplus = self.calculate_total_loss([x],[y])\n",
    "            parameter[ix] = original_value - h\n",
    "            gradminus = self.calculate_total_loss([x],[y])\n",
    "            estimated_gradient = (gradplus - gradminus)/(2*h)\n",
    "            # Reset parameter to original value\n",
    "            parameter[ix] = original_value\n",
    "            # The gradient for this parameter calculated using backpropagation\n",
    "            backprop_gradient = bptt_gradients[pidx][ix]\n",
    "            # calculate The relative error: (|x - y|/(|x| + |y|))\n",
    "            relative_error = np.abs(backprop_gradient - estimated_gradient)/(np.abs(backprop_gradient) + np.abs(estimated_gradient))\n",
    "            # If the error is to large fail the gradient check\n",
    "            if relative_error &gt; error_threshold:\n",
    "                print \"Gradient Check ERROR: parameter=%s ix=%s\" % (pname, ix)\n",
    "                print \"+h Loss: %f\" % gradplus\n",
    "                print \"-h Loss: %f\" % gradminus\n",
    "                print \"Estimated_gradient: %f\" % estimated_gradient\n",
    "                print \"Backpropagation gradient: %f\" % backprop_gradient\n",
    "                print \"Relative Error: %f\" % relative_error\n",
    "                return\n",
    "            it.iternext()\n",
    "        print \"Gradient check for parameter %s passed.\" % (pname)\n",
    " \n",
    "gradient_check = gradient_check\n",
    " \n",
    "# To avoid performing millions of expensive calculations we use a smaller vocabulary size for checking.\n",
    "grad_check_vocab_size = 100\n",
    "np.random.seed(10)\n",
    "model = RNNNumpy(grad_check_vocab_size, 10, bptt_truncate=1000)\n",
    "model.gradient_check([0,1,2,3], [1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
