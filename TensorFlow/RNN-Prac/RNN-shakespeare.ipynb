{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import urllib.request\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.models.rnn.ptb import reader\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data length: 1115394\n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "file_url = 'https://raw.githubusercontent.com/jcjohnson/torch-rnn/master/data/tiny-shakespeare.txt'\n",
    "file_name = 'tinyshakespeare.txt'\n",
    "if not os.path.exists(file_name):\n",
    "    urllib.request.urlretrieve(file_url, file_name)\n",
    "    \n",
    "with open(file_name,'r') as f:\n",
    "    raw_data = f.read()\n",
    "    print(\"Data length:\", len(raw_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Vocab Size is:  65\n",
      "The vocab_to_idx  is:  {',': 0, 'D': 1, 's': 20, 'X': 2, 'I': 4, 'u': 5, 'r': 6, 'h': 51, 'l': 9, 'W': 10, '!': 11, 'w': 12, ';': 40, '3': 15, 'q': 19, '\\n': 18, 'p': 61, 'K': 44, 'G': 21, 'x': 22, 'J': 23, '?': 24, 'P': 25, '.': 26, '-': 27, '$': 32, 'f': 29, ' ': 31, 'L': 33, 'b': 34, 'V': 35, 'o': 30, ':': 36, 'n': 37, 'k': 38, 'e': 39, 'g': 48, 'R': 41, 'Y': 42, 'H': 43, 'A': 45, 'j': 60, 'E': 46, 'c': 47, 'F': 16, 'O': 49, 'N': 50, 't': 17, '&': 28, 'B': 3, 'd': 52, 'z': 13, 'Z': 53, \"'\": 54, 'U': 14, 'y': 56, 'm': 57, 'C': 58, 'S': 59, 'T': 55, 'Q': 8, 'v': 62, 'M': 7, 'a': 63, 'i': 64}\n",
      "The data lenght is:  1115394\n"
     ]
    }
   ],
   "source": [
    "vocab = set(raw_data)\n",
    "vocab_size = len(vocab)\n",
    "idx_to_vocab = dict(enumerate(vocab))\n",
    "vocab_to_idx = dict(zip(idx_to_vocab.values(), idx_to_vocab.keys()))\n",
    "\n",
    "data = [vocab_to_idx[c] for c in raw_data]\n",
    "print('The Vocab Size is: ', vocab_size)\n",
    "print('The vocab_to_idx  is: ', vocab_to_idx)\n",
    "print ('The data lenght is: ', len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 <generator object ptb_iterator at 0x11ef040f8>\n",
      "1 <generator object ptb_iterator at 0x11ef04150>\n",
      "2 <generator object ptb_iterator at 0x11ef040f8>\n",
      "3 <generator object ptb_iterator at 0x11ef04150>\n",
      "4 <generator object ptb_iterator at 0x11ef040f8>\n"
     ]
    }
   ],
   "source": [
    "# Useful Tools:\n",
    "def gen_epochs(n, num_steps, batch_size):\n",
    "    for i in range(n):\n",
    "        yield reader.ptb_iterator(data, batch_size, num_steps)\n",
    "        \n",
    "def reset_graph():  # Reset the graph\n",
    "    if 'sess' in globals() and sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "for idx, epoch in enumerate(gen_epochs(5, 2, 3)):\n",
    "    print (idx, epoch)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_dynamic_rnn_graph_with_list(\n",
    "    state_size = 3, #state_size = 100,   # State size is the number of hidden layer in the hidden unit.\n",
    "    num_classes = 6, #num_classes = vocab_size,\n",
    "    batch_size = 2, #batch_size = 32,\n",
    "    num_steps = 4, #num_steps = 200,  # number of steps is actually no of sequence\n",
    "    learning_rate = 1e-4):\n",
    "\n",
    "    reset_graph()\n",
    "\n",
    "    x = tf.placeholder(tf.int32, [batch_size, num_steps], name='input_placeholder')\n",
    "    y = tf.placeholder(tf.int32, [batch_size, num_steps], name='labels_placeholder')\n",
    "\n",
    "#     x_one_hot = tf.one_hot(x, num_classes)\n",
    "#     print ('The One Hot vector is: ', x_one_hot)\n",
    "#     rnn_inputs = [tf.squeez e(i,squeeze_dims=[1]) for i in tf.split(1, num_steps, x_one_hot)]\n",
    "\n",
    "#     cell = tf.nn.rnn_cell.LSTMCell(state_size, state_is_tuple=True)\n",
    "#     init_state = cell.zero_state(batch_size, tf.float32)\n",
    "#     rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, rnn_inputs, initial_state=init_state)\n",
    "    \n",
    "    # For a Dynamic_RNN we need the input in the form of a matrix\n",
    "    embeddings = tf.get_variable('embedding_matrix', [num_classes, state_size])\n",
    "    rnn_inputs = tf.nn.embedding_lookup(embeddings, x)\n",
    "    \n",
    "    # The process from hidden layer to the output layer.\n",
    "    cell = tf.nn.rnn_cell.LSTMCell(state_size, state_is_tuple=True)\n",
    "    init_state = cell.zero_state(batch_size, tf.float32)\n",
    "    rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, rnn_inputs, initial_state=init_state)\n",
    "    \n",
    "#     rnn_outputs: [no_of_batch x no_of_sequences x no_of_hidden_units]\n",
    "    with tf.variable_scope('softmax'):\n",
    "        W = tf.get_variable('W', [state_size, num_classes]) # no_of_hidden_layers x no_of_classes\n",
    "        b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(0.0))\n",
    "        \n",
    "    \n",
    "#     W = tf.get_variable('W', [state_size, num_classes]) # no_of_hidden_layers x no_of_classes\n",
    "#     b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(0.0))\n",
    "        \n",
    "        \n",
    "        \n",
    "    rnn_outputs_new = tf.reshape(rnn_outputs, [-1, state_size]) \n",
    "    # [no_of_batch x no_of_sequences x no_of_hidden_units]\n",
    "    y_reshaped = tf.reshape(y, [-1])\n",
    "\n",
    "    logits = tf.matmul(rnn_outputs_new, W) + b\n",
    "    \n",
    "    # logits : [no_of_sequences x no_of_hidden_units] x [no_of_hidden_layers x no_of_classes] + \n",
    "\n",
    "#     y_as_list = [tf.squeeze(i, squeeze_dims=[1]) for i in tf.split(1, num_steps, y)]\n",
    "\n",
    "#     loss_weights = [tf.ones([batch_size]) for i in range(num_steps)]\n",
    "#     losses = tf.nn.seq2seq.sequence_loss_by_example(logits, y_as_list, loss_weights)\n",
    "#     total_loss = tf.reduce_mean(losses)\n",
    "#     train_step = tf.train.AdamOptimizer(learning_rate).minimize(total_loss)\n",
    "\n",
    "#     return dict(\n",
    "#         x = x,\n",
    "#         y = y,\n",
    "#         init_state = init_state,\n",
    "#         final_state = final_state,\n",
    "#         total_loss = total_loss,\n",
    "#         train_step = train_step\n",
    "#     )\n",
    "\n",
    "    return dict(\n",
    "        x=x,\n",
    "        y=y,\n",
    "        embeddings = embeddings,\n",
    "        rnn_inputs = rnn_inputs,\n",
    "        init_state = init_state,\n",
    "        rnn_outputs = rnn_outputs,\n",
    "        final_state = final_state,\n",
    "        rnn_outputs_new = rnn_outputs_new,\n",
    "        logits = logits,\n",
    "        W = W\n",
    "    )\n",
    "\n",
    "g = build_dynamic_rnn_graph_with_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings [[ -2.23231614e-01   3.97504985e-01  -4.65226859e-01]\n",
      " [  9.35203433e-02   1.53714418e-01   2.30824292e-01]\n",
      " [ -8.26793313e-02  -4.87440944e-01  -4.58240509e-04]\n",
      " [ -5.99353313e-02   1.21155381e-02   4.58998263e-01]\n",
      " [  5.91485500e-02  -2.89413124e-01  -6.78060651e-01]\n",
      " [ -6.63731456e-01  -6.24574482e-01   4.48079288e-01]]\n",
      "\n",
      "rnn_inputs  [[[  9.35203433e-02   1.53714418e-01   2.30824292e-01]\n",
      "  [ -8.26793313e-02  -4.87440944e-01  -4.58240509e-04]\n",
      "  [ -5.99353313e-02   1.21155381e-02   4.58998263e-01]\n",
      "  [  5.91485500e-02  -2.89413124e-01  -6.78060651e-01]]\n",
      "\n",
      " [[  9.35203433e-02   1.53714418e-01   2.30824292e-01]\n",
      "  [ -5.99353313e-02   1.21155381e-02   4.58998263e-01]\n",
      "  [  5.91485500e-02  -2.89413124e-01  -6.78060651e-01]\n",
      "  [ -2.23231614e-01   3.97504985e-01  -4.65226859e-01]]]\n",
      "\n",
      "init_state  LSTMStateTuple(c=array([[ 0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.]], dtype=float32), h=array([[ 0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.]], dtype=float32))\n",
      "\n",
      "rnn_outputs  [[[-0.01344888  0.03842906 -0.03434123]\n",
      "  [-0.00556574 -0.03229914  0.06478789]\n",
      "  [-0.0563427   0.01090476  0.03205518]\n",
      "  [ 0.02435422 -0.13242263  0.10658354]]\n",
      "\n",
      " [[-0.01344888  0.03842906 -0.03434123]\n",
      "  [-0.04707175  0.06500771 -0.04493336]\n",
      "  [ 0.03989734 -0.0827466   0.03594165]\n",
      "  [ 0.04566928 -0.00311378 -0.00795503]]]\n",
      "\n",
      "final_state  LSTMStateTuple(c=array([[ 0.07273715, -0.24174175,  0.21187213],\n",
      "       [ 0.09482969, -0.00506228, -0.01817069]], dtype=float32), h=array([[ 0.02435422, -0.13242263,  0.10658354],\n",
      "       [ 0.04566928, -0.00311378, -0.00795503]], dtype=float32))\n",
      "\n",
      "rnn_outputs_new  [[-0.01344888  0.03842906 -0.03434123]\n",
      " [-0.00556574 -0.03229914  0.06478789]\n",
      " [-0.0563427   0.01090476  0.03205518]\n",
      " [ 0.02435422 -0.13242263  0.10658354]\n",
      " [-0.01344888  0.03842906 -0.03434123]\n",
      " [-0.04707175  0.06500771 -0.04493336]\n",
      " [ 0.03989734 -0.0827466   0.03594165]\n",
      " [ 0.04566928 -0.00311378 -0.00795503]]\n",
      "\n",
      "logits  [[-0.04224116 -0.03650276 -0.01887563  0.00541336  0.01558777  0.04422517]\n",
      " [ 0.07910316  0.02985683  0.03903275 -0.00199502 -0.02055291 -0.06334142]\n",
      " [ 0.06285852 -0.03275103  0.01822965  0.01668293 -0.02921296 -0.00227747]\n",
      " [ 0.15145828  0.10813107  0.05464507 -0.00931014 -0.07215164 -0.13076942]\n",
      " [-0.04224116 -0.03650276 -0.01887563  0.00541336  0.01558777  0.04422517]\n",
      " [-0.04218754 -0.07387121 -0.02530318  0.01680678  0.00987151  0.07384216]\n",
      " [ 0.04989496  0.07496979  0.01542743 -0.01180305 -0.03092263 -0.06426211]\n",
      " [-0.03858998  0.02896464 -0.00134367 -0.01583333  0.03096864 -0.01453832]]\n",
      "\n",
      "weights  [[-0.73299932  0.65155649  0.1024766  -0.37005639  0.75888705 -0.47492075]\n",
      " [-0.57875514 -0.46891975  0.16399121 -0.08037496  0.80430031  0.13433075]\n",
      " [ 0.86945534  0.28303933  0.69302893 -0.1026535   0.14893317 -0.95150399]]\n",
      "\n",
      "\n",
      "popopoopopopopooopoopopopopoopooopo\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sam/App-Setup/CondaENV/lib/python3.5/site-packages/ipykernel/__main__.py:55: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-89-dd9bfd241d0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m \u001b[0mtrain_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"It took\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"seconds to train for 3 epochs.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-89-dd9bfd241d0b>\u001b[0m in \u001b[0;36mtrain_network\u001b[0;34m(g, num_epochs, num_steps, batch_size, verbose, save)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mx_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0my_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_new\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;31m#         a , b, c, d, e, f, g, w = sess.run([g['embeddings'],\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;31m#                                    g['rnn_inputs'],\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "def train_network(g, num_epochs, num_steps = 200, batch_size = 32, verbose = True, save=False):\n",
    "    tf.set_random_seed(2345)  # We set the random seed to track the same random chosen datapoints\n",
    "    with tf.Session() as sess: # Open the session\n",
    "        sess.run(tf.initialize_all_variables())  # Initialize all the variables\n",
    "#         training_losses = []\n",
    "#         for idx, epoch in enumerate(gen_epochs(num_epochs, num_steps, batch_size)):\n",
    "#             training_loss = 0\n",
    "#             steps = 0\n",
    "#             training_state = None\n",
    "#             print (epoch)\n",
    "#             for X, Y in epoch:\n",
    "#                 steps += 1\n",
    "#                 # X is the ids with the sequence of the sentence and Y is the prediction sequnence\n",
    "#                 print (len([idx_to_vocab[ch] for ch in X[0]]))\n",
    "#                 print (len([idx_to_vocab[ch] for ch in X[1]]))\n",
    "#                 print ('')\n",
    "#                 print (len([idx_to_vocab[ch] for ch in Y[0]]))\n",
    "#                 print (len([idx_to_vocab[ch] for ch in Y[1]]))\n",
    "        x = np.array([[1,2,3,4],[1,3,4,0]])\n",
    "        y = np.array([[2,3,4,5],[3,4,0,5]])\n",
    "        feed_dict={g['x']: x, g['y']: y}\n",
    "        a , b, c, d, e, f, g, w = sess.run([g['embeddings'], \n",
    "                                   g['rnn_inputs'], \n",
    "                                   g['init_state'], \n",
    "                                   g['rnn_outputs'], \n",
    "                                   g['final_state'],\n",
    "                                   g['rnn_outputs_new'],\n",
    "                                   g['logits'],\n",
    "                                   g['W']],  feed_dict)\n",
    "        print ('embeddings', a) \n",
    "        print ('')\n",
    "        print ('rnn_inputs ', b)\n",
    "        print ('')\n",
    "        print ('init_state ', c)\n",
    "        print ('')\n",
    "        print ('rnn_outputs ', d)\n",
    "        print ('')\n",
    "        print ('final_state ', e)\n",
    "        print ('')\n",
    "        print ('rnn_outputs_new ', f)\n",
    "        print ('')\n",
    "        print ('logits ', g)\n",
    "        print ('')\n",
    "        print ('weights ', w)\n",
    "\n",
    "        \n",
    "        print ('')\n",
    "        print ('')\n",
    "        print ('popopoopopopopooopoopopopopoopooopo')\n",
    "        print ('')\n",
    "        print ('')\n",
    "        \n",
    "        x_new = np.array([[1,4,2,2],[1,4,3,0]])\n",
    "        y_new = np.array([[4,2,2,5],[4,3,0,5]])\n",
    "        feed_dict={g['x']: x_new, g['y']: y_new}\n",
    "#         a , b, c, d, e, f, g, w = sess.run([g['embeddings'], \n",
    "#                                    g['rnn_inputs'], \n",
    "#                                    g['init_state'], \n",
    "#                                    g['rnn_outputs'], \n",
    "#                                    g['final_state'],\n",
    "#                                    g['rnn_outputs_new'],\n",
    "#                                    g['logits'],\n",
    "#                                    g['W']],  feed_dict)\n",
    "#         print ('embeddings', a) \n",
    "#         print ('')\n",
    "#         print ('rnn_inputs ', b)\n",
    "#         print ('')\n",
    "#         print ('init_state ', c)\n",
    "#         print ('')\n",
    "#         print ('rnn_outputs ', d)\n",
    "#         print ('')\n",
    "#         print ('final_state ', e)\n",
    "#         print ('')\n",
    "#         print ('rnn_outputs_new ', f)\n",
    "#         print ('')\n",
    "#         print ('logits ', g)\n",
    "#         print ('')\n",
    "#         print ('weights ', w)\n",
    "\n",
    "        \n",
    "        \"\"\"\n",
    "        embeddings: Is basically the weights from each words from the vocabulay list to all the hidden units.\n",
    "        rnn_inputs: This takes the weights from embeddings for the corresponding input sequence.\n",
    "                    Is the unput from each sequence to the hidden layer \n",
    "                    shape = [batch_size x no_of_sequence x no_of hidden_inits]\n",
    "        \n",
    "        \"\"\"\n",
    "#                 print (sess.run(x_one_hot))\n",
    "# #                 print (g['x_one_hot'].eval())\n",
    "#                 dictionary = sess.run(feed_dict)\n",
    "#                 a = g['x_one_hot'].eval()\n",
    "# #                 print ([w for no, w in enumerate(g['x_one_hot'])])\n",
    "# #                 print (g['rnn_inputs'])\n",
    "\n",
    "#                 break\n",
    "#                 if training_state is not None:\n",
    "#                     feed_dict[g['init_state']] = training_state\n",
    "#                 training_loss_, training_state, _ = sess.run([g['total_loss'],\n",
    "#                                                       g['final_state'],\n",
    "#                                                       g['train_step']],\n",
    "#                                                              feed_dict)\n",
    "#                 training_loss += training_loss_\n",
    "#             if verbose:\n",
    "#                 print(\"Average training loss for Epoch\", idx, \":\", training_loss/steps)\n",
    "#             training_losses.append(training_loss/steps)\n",
    "\n",
    "#         if isinstance(save, str):\n",
    "#             g['saver'].save(sess, save)\n",
    "\n",
    "#     return training_losses\n",
    "\n",
    "# t = time.time()\n",
    "# build_dynamic_rnn_graph_with_list()\n",
    "# print(\"It took\", time.time() - t, \"seconds to build the graph.\")\n",
    "\n",
    "t = time.time()\n",
    "train_network(g, 3)\n",
    "print(\"It took\", time.time() - t, \"seconds to train for 3 epochs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights  [[ 0.43188667 -0.63292527  0.77183008 -0.97397757  0.216676   -0.11577225]\n",
    " [ 0.80125594  0.91688633  0.74394011  0.86566377 -0.27232838  0.01420283]\n",
    " [-0.91036487 -0.96914315 -0.43888903  0.78052235  0.54020953 -0.86834955]]\n",
    "It took 0.07764196395874023 seconds to train for 3 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_multilayer_lstm_graph_with_dynamic_rnn(\n",
    "    state_size = 100,\n",
    "    num_classes = vocab_size,\n",
    "    batch_size = 32,\n",
    "    num_steps = 200,\n",
    "    num_layers = 3,\n",
    "    learning_rate = 1e-4):\n",
    "\n",
    "    reset_graph()\n",
    "\n",
    "    x = tf.placeholder(tf.int32, [batch_size, num_steps], name='input_placeholder')\n",
    "    y = tf.placeholder(tf.int32, [batch_size, num_steps], name='labels_placeholder')\n",
    "\n",
    "    embeddings = tf.get_variable('embedding_matrix', [num_classes, state_size])\n",
    "\n",
    "    # Note that our inputs are no longer a list, but a tensor of dims batch_size x num_steps x state_size\n",
    "    rnn_inputs = tf.nn.embedding_lookup(embeddings, x)\n",
    "\n",
    "    cell = tf.nn.rnn_cell.LSTMCell(state_size, state_is_tuple=True)\n",
    "    cell = tf.nn.rnn_cell.MultiRNNCell([cell] * num_layers, state_is_tuple=True)\n",
    "    init_state = cell.zero_state(batch_size, tf.float32)\n",
    "    rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, rnn_inputs, initial_state=init_state)\n",
    "\n",
    "    with tf.variable_scope('softmax'):\n",
    "        W = tf.get_variable('W', [state_size, num_classes])\n",
    "        b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "    #reshape rnn_outputs and y so we can get the logits in a single matmul\n",
    "    rnn_outputs = tf.reshape(rnn_outputs, [-1, state_size])\n",
    "    y_reshaped = tf.reshape(y, [-1])\n",
    "\n",
    "    logits = tf.matmul(rnn_outputs, W) + b\n",
    "\n",
    "    total_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits, y_reshaped))\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate).minimize(total_loss)\n",
    "\n",
    "    return dict(\n",
    "        x = x,\n",
    "        y = y,\n",
    "        init_state = init_state,\n",
    "        final_state = final_state,\n",
    "        total_loss = total_loss,\n",
    "        train_step = train_step\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 0.6965160369873047 seconds to build the graph.\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "build_multilayer_lstm_graph_with_dynamic_rnn()\n",
    "print(\"It took\", time.time() - t, \"seconds to build the graph.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss for Epoch 0 : 3.50457037043\n",
      "Average training loss for Epoch 1 : 3.3163155057\n",
      "Average training loss for Epoch 2 : 3.25275716288\n",
      "It took 478.65483713150024 seconds to train for 3 epochs.\n"
     ]
    }
   ],
   "source": [
    "g = build_multilayer_lstm_graph_with_dynamic_rnn()\n",
    "t = time.time()\n",
    "train_network(g, 3)\n",
    "print(\"It took\", time.time() - t, \"seconds to train for 3 epochs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
