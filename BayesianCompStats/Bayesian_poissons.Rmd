---
title: "Bayesian_poissons"
output: pdf_document
---


```{r}
library("COUNT")

data("badhealth")
head(badhealth)
```

```{r}
# Check for missing value and distribution statistics
any(is.na(badhealth))
```

# Plot th ehistogram of th response variable: See, sheavoly right skewed
```{r}
hist(badhealth$numvisit, breaks=20)
```

Plot numvisit against the explanatory variables, add jitters:
Since we are modeling poissons distribution, we would like to use the log scale. Some counts are 0 in the response variable, that would not produce correct log values. 

1. It seems to some extent that when age increases than the number of visits increases. 
2. The red points (bad_heath) are more on the top region, which makes sense because if people have ba health, they tend to visit to hospital more often.
```{r}
# plot numvisits against age where badh is 0 and numvisits>0
plot(jitter(log(numvisit)) ~ jitter(age), data=badhealth, subset=badh==0&numvisit>0, xlab="age", ylab="log(numvisits)")
# plot numvisits against age where badh is 0 and numvisits>0
points(jitter(log(numvisit)) ~ jitter(age), data=badhealth, subset=badh==1&numvisit>0, xlab="age", ylab="log(numvisits)", col="red")
```


### Modeling: He have the following.
Here we would also like to add a interaction variable between badh and age.
1. likelihood : y|x,b0,b1,b2 ~ Poissons(lambda)
2. Prior: b0 ~ N(0,inf)
          b_badh ~ N(0, inf)
          b_age ~ N(0, inf)
          b_interaction ~ N(0, inf)

Benefit: Teh benifit of poissons model in that unlike the log lnear model where we take a log over the response y. In the poissons distribution we take the log over the link function i.e the parameters lamda.  
```{r}
mod1_string = "model{
  for (i in 1:length(numvisit)){
    numvisit[i] ~ dpois(lam[i])
    log(lam[i]) = b0 + b_badh*badh[i] + b_age*age[i] + b_interaction*age[i]*badh[i]
  }
  
  b0 ~ dnorm(0.0, 1/1e6)
  b_badh ~ dnorm(0.0, 1/1e4)
  b_age ~ dnorm(0.0, 1/1e4)
  b_interaction ~ dnorm(0.0, 1/1e4)
}"
```

### Set up experiment
```{r}
library("rjags")

set.seed(102)
data_jags = as.list(badhealth)
str(data_jags)

params = c('b0', 'b_badh', 'b_age', 'b_interaction')

## Initialize the model
mod = jags.model(textConnection(mod1_string), data=data_jags, n.chains=3)

## Run the model for 1000 burning period
update(mod, 1e3)

## Run and save for another 5000 simulations
mod_sim = coda.samples(model=mod, variable.names = params, n.iter=5e3)

## Combine the simulation results of parameters
mod_csim = as.mcmc(do.call(rbind, mod_sim))
```

Diagnoitics:
```{r, fig.width=14, fig.height=15}
plot(mod_sim)
```


```{r}
gelman.diag(mod1_sim)
```

```{r}
autocorr.diag(mod_sim)
autocorr.plot(mod_sim)
```


```{r}
dic1 = dic.samples(mod, n.iter=1e3)

dic1
effectiveSize(mod_sim)
```

### Residual analysis
```{r}
X = as.matrix(badhealth[,-1])
head(X)
X = cbind(X, with(badhealth, badh*age))
head(X)

# Check out the posterior median for each parameters. We could even take the means
pmed_coef = apply(mod_csim, 2, median) # Here 2 indicates perform column wise operation
pmed_coef

# We have the params and we have the X, let compute the linear model
log_lambda = pmed_coef['b0'] + X %*% pmed_coef[c("b_badh", "b_age", "b_interaction")]
head(log_lambda)

# Since we took logarithm of lamda which is the parameter for Y, Here we take the exponential of the logarithmic scale to 
# get back our actual predicted y_hat
lamda_hat = exp(log_lambda)
head(lamda_hat)

# Residuals
resid = badhealth$numvisit - lamda_hat
set.seed(001)
plot(sample(resid)) # We shuffle beacuse the y's were sorted by num_visits. If not shuffled the plot would look confusing


```




Plotting Residual Vs lambda_hat:
1. It can be seen that the mean of doctor visits when the health is not bad is 2
    and the mean of doctor visits when the health is bad is 6. This makes sense
2. 
```{r, fig.width=8, fig.height=10}
plot(lamda_hat[which(badhealth$badh==0)], resid[which(badhealth$badh==0)], xlim=c(0,8), ylim = range(resid))
points(lamda_hat[which(badhealth$badh==1)], resid[which(badhealth$badh==1)], col='red')
```


In the poissons model, the parameter lamda is the mean and variance. Teh above plot make sense that the variability increases as we move from no bad health to bad health. But lets check the variance of the teh residuals. It seems that the variability of the residuals is too high than expected. This indicated the model might have fit poorly which means that the covariated (badh, age, interaction) do not explain very well the variability in the data or the data are overdisperesed for the poissons likelihood. Inthe later case it is often good to try other models such as negative binomial distribution.
```{r}
var(resid[which(badhealth$badh==0)])

var(resid[which(badhealth$badh==1)])
```

### Interpretig Summary Results:
WE saw that the model fit was not adequate. But let assume it was and lets interpret the outputs.

If the parameters are positive, then there is a positive assosiation between that parameter and doctor visits

1. b0 is not interpretable because in the data set we have 20 years as minimum healthy individual and here b0 has value closer to 0.
2. b_age: Age shows a positive weight for number of doctor visits and it makes sense becasue even if you are older, you are still probable to visit doctors.
3. b_badh: A positive effect, if bad health than more probale to visit a doctor
3. b_interaction: The output has modeled number of visits. The interaction coefficient is negative, the b_interaction is interpreted as an adjustment for people with less age but bad health. This makes sense because even your age is not very high and you have bad health, you would still visit a doctor.

```{r}
summary(mod_sim)
```

# Why Bayesian Analysis is Awesome:
Lets say we have two people, one is in good health and the other is in bad health. We would be intereseted in a (Posterior Probability) that the indevidual would have more doctor visits. Scenario's like these are where posterior hypothesis sines.

These interpretations are difficult with frequentist approach, A frequentist apprach would simply say the number of visits on average given the patient condition. In bayesian we can get a estimated range.

Say we have the below individuals
    age  badh interaction
x1:  35    0      0
x2:  35    1      1


Idea: The idea to calculate the probability is:
Step 1. Compute the posterior distribution using MCMC samples and likelihood linear model. Compute the loglamda given sampled MCMC parameters (b0, b_age, b_badh, b_interaction) which is simply the linear combination. i.e [b1 + b1x1+ ...]
Step 2: Calculate the exponent of log lambda to bring them the prediction scale
Step 3: For each person, Sample "number of visits" from the poissons distribution, where the mean is lamda drawn from the posterior distribution and exponentiated.

THIS IS BASICALLY THE POSTERIOR PREDICTIVE RESPONSE


```{r}
x1 = c(35, 0, 0)
x2 = c(35, 1, 35)

# The posteriar distribution for the parameters looks like
head(mod_csim)

# Compute the posterior density for the two individual
# Compute the linear term i.e b0 + b1x1 + b2x2 .....
loglam1 = mod_csim[, "b0"] + mod_csim[, c(2,3,4)] %*% x1 
loglam2 = mod_csim[, "b0"] + mod_csim[, c(2,3,4)] %*% x2

# We have the samples form the posterior distributino of log on lambda of poissons model. We now, calculate the exponent to bring them to the right scale.
lam1 = exp(loglam1)
lam2 = exp(loglam2)

# For the above step we have montecarlo means for the two individual, We now vizuallize them
plot(density(lam1))
plot(density(lam2))

# From the inference:
# From the plots below, we can say that for person1 (with good health) we can assume their doctor visits to range from 1.8 to 2.1, whereas for person 2 its between 5.5 to 7. This makes sense

# Now even furter we determin the posterior probability of person 1 having more doctor visits.
# Here we simulate "lamdas"" from the posterior distribution of lambdas of both the person.
y1 = rpois(length(lam1), lam1)
y2 = rpois(length(lam2), lam2)
```


```{r}
head(y1)
head(y2)

# We compute probabilities and plot the number of doctor visits against its probility. As you can see the plots make complete sense. 
#### NOTE the levels below shows the number of doctor visits.
plot(table(factor(y1, levels=0:20))/length(y1), xlab = 'Number of doctor visits (good health)', ylab = 'Probability of doctor visits (good health)')
points(table((y2+0.1))/length(y2), xlab = 'Number of doctor visits (bad health)', ylab = 'Probability of doctor visits (bad health)', col='red')

# Finally answer to outquestion:
# What is the probability of doctor visits of person 1 will be more than person 2
mean(y2>y1)  # 91.6 % which makes complete sense

```

## Final theory:
Beacasue we used the posterior samples for our model parameters in the simulations. This posterior predictive distribution on the "number of doctor visits." for each person naturally takes in account of the uncertainity of our model estimates. This creates a much honest and reliable output that we would get we had fixd the model parameter if we had fixed our b's or model parameters to a fixed number.


